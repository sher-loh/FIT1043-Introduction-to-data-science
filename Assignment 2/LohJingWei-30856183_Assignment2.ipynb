{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7ff1d6e",
   "metadata": {},
   "source": [
    "<div style=\"font-size:15pt;margin-bottom:0;text-align: right\"><b>FIT1043 Introduction to Data Science</b></div>\n",
    "<div style=\"font-size:15pt;margin-top:0;text-align: right\"><b>Assignment 2</b></div>\n",
    "<br>\n",
    "<div style=\"text-align: right\"> Loh Jing Wei </div>\n",
    "<div style=\"text-align: right\"> 30856183 </div>\n",
    "<div style=\"text-align: right\"> <i> 29th April 2022 </i> </div>\n",
    "<hr style=\"border:0.5px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc87115c",
   "metadata": {},
   "source": [
    "***\n",
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619244fa",
   "metadata": {},
   "source": [
    "This assignment involves using data about essays from a CSV file named FIT1043-Essay-Features.csv to create a model which will predict the scores of the essays. <br>\n",
    "The data we will be dealing with is labelled data, hence, supervised machine learning method will be used. <br>\n",
    "The label for this dataset will be the score of the essays which is a categorical-ordinal data type, meanwhile features will be chosen using filter selection method. After choosing our features, we split the dataset into testing and training datasets. <br>\n",
    "We will normalize and scale the data using StandardScaler. Subsequently, Support vector machine (specifically SVC) is chosen as the main algorithm to form our model. <br>\n",
    "Confusion matrix, accuracy score and Quadratic Weighted Kappa (QWK) are used to assess our model. <br>\n",
    "The model will then be used to predict scores for dataset from another CSV file named FIT1043-Essay-Features-Submission.csv, and the result (predicted scores for the essays) will be exported into a CSV file called 30856183-Jing-Wei-Loh-1.csv. This CSV file will be uploaded to Kaggle.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8cfdb2",
   "metadata": {},
   "source": [
    "### Importing the necessary libraries and reading files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093028ad",
   "metadata": {},
   "source": [
    "We import the library <b>pandas</b> to use the data structure DataFrame and the related functions to read data from CSV files as well as data manipulation and analysis. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9931a248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3a15f6",
   "metadata": {},
   "source": [
    "<b>FIT1043-Essay-Features.csv</b> file is read and stored as DataFrame with name essay_feature. <br>\n",
    "This FIT1043-Essay-Features.csv file describes features from a set of essays and provide information about the essays.<br>\n",
    "<b>30856183-Jing-Wei-Loh-1.csv</b> file is read and stored as DataFrame named submission. It will be used to store the predicted score for Kaggle submission. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "872081c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_features = pd.read_csv('FIT1043-Essay-Features.csv')\n",
    "submission = pd.read_csv('30856183-Jing-Wei-Loh-1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0945f74",
   "metadata": {},
   "source": [
    "Check for the number of columns and rows of essay_features DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed9bdcaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1332, 19)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essay_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260493bd",
   "metadata": {},
   "source": [
    "There are 1332 rows and 19 columns in total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe21ed4",
   "metadata": {},
   "source": [
    "Check for the first 5 entries of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d91d7fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essayid</th>\n",
       "      <th>chars</th>\n",
       "      <th>words</th>\n",
       "      <th>commas</th>\n",
       "      <th>apostrophes</th>\n",
       "      <th>punctuations</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>sentences</th>\n",
       "      <th>questions</th>\n",
       "      <th>avg_word_sentence</th>\n",
       "      <th>POS</th>\n",
       "      <th>POS/total_words</th>\n",
       "      <th>prompt_words</th>\n",
       "      <th>prompt_words/total_words</th>\n",
       "      <th>synonym_words</th>\n",
       "      <th>synonym_words/total_words</th>\n",
       "      <th>unstemmed</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1457</td>\n",
       "      <td>2153</td>\n",
       "      <td>426</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>5.053991</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>26.625000</td>\n",
       "      <td>423.995272</td>\n",
       "      <td>0.995294</td>\n",
       "      <td>207</td>\n",
       "      <td>0.485915</td>\n",
       "      <td>105</td>\n",
       "      <td>0.246479</td>\n",
       "      <td>424</td>\n",
       "      <td>412</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>503</td>\n",
       "      <td>1480</td>\n",
       "      <td>292</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>5.068493</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>26.545455</td>\n",
       "      <td>290.993103</td>\n",
       "      <td>0.996552</td>\n",
       "      <td>148</td>\n",
       "      <td>0.506849</td>\n",
       "      <td>77</td>\n",
       "      <td>0.263699</td>\n",
       "      <td>356</td>\n",
       "      <td>345</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>253</td>\n",
       "      <td>3964</td>\n",
       "      <td>849</td>\n",
       "      <td>19</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>4.669022</td>\n",
       "      <td>49</td>\n",
       "      <td>2</td>\n",
       "      <td>17.326531</td>\n",
       "      <td>843.990544</td>\n",
       "      <td>0.994100</td>\n",
       "      <td>285</td>\n",
       "      <td>0.335689</td>\n",
       "      <td>130</td>\n",
       "      <td>0.153121</td>\n",
       "      <td>750</td>\n",
       "      <td>750</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>107</td>\n",
       "      <td>988</td>\n",
       "      <td>210</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>4.704762</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>207.653784</td>\n",
       "      <td>0.988828</td>\n",
       "      <td>112</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>62</td>\n",
       "      <td>0.295238</td>\n",
       "      <td>217</td>\n",
       "      <td>209</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1450</td>\n",
       "      <td>3139</td>\n",
       "      <td>600</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>5.231667</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>594.652150</td>\n",
       "      <td>0.991087</td>\n",
       "      <td>255</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>165</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>702</td>\n",
       "      <td>677</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essayid  chars  words  commas  apostrophes  punctuations  avg_word_length  \\\n",
       "0     1457   2153    426      14            6             0         5.053991   \n",
       "1      503   1480    292       9            7             0         5.068493   \n",
       "2      253   3964    849      19           26             1         4.669022   \n",
       "3      107    988    210       8            7             0         4.704762   \n",
       "4     1450   3139    600      13            8             0         5.231667   \n",
       "\n",
       "   sentences  questions  avg_word_sentence         POS  POS/total_words  \\\n",
       "0         16          0          26.625000  423.995272         0.995294   \n",
       "1         11          0          26.545455  290.993103         0.996552   \n",
       "2         49          2          17.326531  843.990544         0.994100   \n",
       "3         12          0          17.500000  207.653784         0.988828   \n",
       "4         24          1          25.000000  594.652150         0.991087   \n",
       "\n",
       "   prompt_words  prompt_words/total_words  synonym_words  \\\n",
       "0           207                  0.485915            105   \n",
       "1           148                  0.506849             77   \n",
       "2           285                  0.335689            130   \n",
       "3           112                  0.533333             62   \n",
       "4           255                  0.425000            165   \n",
       "\n",
       "   synonym_words/total_words  unstemmed  stemmed  score  \n",
       "0                   0.246479        424      412      4  \n",
       "1                   0.263699        356      345      4  \n",
       "2                   0.153121        750      750      4  \n",
       "3                   0.295238        217      209      3  \n",
       "4                   0.275000        702      677      4  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essay_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ac2f8e",
   "metadata": {},
   "source": [
    "Use .describe() function to find the count, mean, standard deviation, minimum, 25th percentile, 50th percentile, 75th percentile and maximum value for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e73bede5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essayid</th>\n",
       "      <th>chars</th>\n",
       "      <th>words</th>\n",
       "      <th>commas</th>\n",
       "      <th>apostrophes</th>\n",
       "      <th>punctuations</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>sentences</th>\n",
       "      <th>questions</th>\n",
       "      <th>avg_word_sentence</th>\n",
       "      <th>POS</th>\n",
       "      <th>POS/total_words</th>\n",
       "      <th>prompt_words</th>\n",
       "      <th>prompt_words/total_words</th>\n",
       "      <th>synonym_words</th>\n",
       "      <th>synonym_words/total_words</th>\n",
       "      <th>unstemmed</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1332.00000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.00000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.00000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>905.27027</td>\n",
       "      <td>2101.745495</td>\n",
       "      <td>424.485736</td>\n",
       "      <td>14.667417</td>\n",
       "      <td>8.141141</td>\n",
       "      <td>0.47973</td>\n",
       "      <td>4.939762</td>\n",
       "      <td>19.704204</td>\n",
       "      <td>1.222973</td>\n",
       "      <td>23.884687</td>\n",
       "      <td>420.596542</td>\n",
       "      <td>0.989935</td>\n",
       "      <td>198.913664</td>\n",
       "      <td>0.469164</td>\n",
       "      <td>110.16967</td>\n",
       "      <td>0.263846</td>\n",
       "      <td>468.987988</td>\n",
       "      <td>455.507508</td>\n",
       "      <td>3.427177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>526.68760</td>\n",
       "      <td>865.963750</td>\n",
       "      <td>171.873730</td>\n",
       "      <td>10.920781</td>\n",
       "      <td>6.124520</td>\n",
       "      <td>1.27168</td>\n",
       "      <td>0.231071</td>\n",
       "      <td>19.202731</td>\n",
       "      <td>1.847446</td>\n",
       "      <td>11.160020</td>\n",
       "      <td>170.985111</td>\n",
       "      <td>0.007308</td>\n",
       "      <td>82.729266</td>\n",
       "      <td>0.052466</td>\n",
       "      <td>43.96192</td>\n",
       "      <td>0.038870</td>\n",
       "      <td>159.447449</td>\n",
       "      <td>155.751220</td>\n",
       "      <td>0.774275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.231322</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.084112</td>\n",
       "      <td>35.647059</td>\n",
       "      <td>0.924771</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>11.00000</td>\n",
       "      <td>0.027299</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>442.75000</td>\n",
       "      <td>1527.250000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.791679</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.142857</td>\n",
       "      <td>305.406284</td>\n",
       "      <td>0.987758</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>0.435709</td>\n",
       "      <td>81.00000</td>\n",
       "      <td>0.238423</td>\n",
       "      <td>361.000000</td>\n",
       "      <td>350.750000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>914.50000</td>\n",
       "      <td>2029.500000</td>\n",
       "      <td>411.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.946059</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>22.030331</td>\n",
       "      <td>406.982869</td>\n",
       "      <td>0.991572</td>\n",
       "      <td>193.000000</td>\n",
       "      <td>0.465852</td>\n",
       "      <td>107.50000</td>\n",
       "      <td>0.262872</td>\n",
       "      <td>463.000000</td>\n",
       "      <td>448.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1369.75000</td>\n",
       "      <td>2613.500000</td>\n",
       "      <td>525.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>5.092938</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>26.048234</td>\n",
       "      <td>520.739458</td>\n",
       "      <td>0.994425</td>\n",
       "      <td>246.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>134.00000</td>\n",
       "      <td>0.288277</td>\n",
       "      <td>581.000000</td>\n",
       "      <td>561.250000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1799.00000</td>\n",
       "      <td>6142.000000</td>\n",
       "      <td>1170.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>26.00000</td>\n",
       "      <td>5.681429</td>\n",
       "      <td>642.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>1158.984563</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>669.000000</td>\n",
       "      <td>0.961207</td>\n",
       "      <td>355.00000</td>\n",
       "      <td>0.465517</td>\n",
       "      <td>750.000000</td>\n",
       "      <td>750.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          essayid        chars        words       commas  apostrophes  \\\n",
       "count  1332.00000  1332.000000  1332.000000  1332.000000  1332.000000   \n",
       "mean    905.27027  2101.745495   424.485736    14.667417     8.141141   \n",
       "std     526.68760   865.963750   171.873730    10.920781     6.124520   \n",
       "min       0.00000   169.000000    36.000000     0.000000     2.000000   \n",
       "25%     442.75000  1527.250000   310.000000     7.000000     4.000000   \n",
       "50%     914.50000  2029.500000   411.000000    13.000000     6.000000   \n",
       "75%    1369.75000  2613.500000   525.000000    21.000000    11.000000   \n",
       "max    1799.00000  6142.000000  1170.000000    72.000000    51.000000   \n",
       "\n",
       "       punctuations  avg_word_length    sentences    questions  \\\n",
       "count    1332.00000      1332.000000  1332.000000  1332.000000   \n",
       "mean        0.47973         4.939762    19.704204     1.222973   \n",
       "std         1.27168         0.231071    19.202731     1.847446   \n",
       "min         0.00000         2.231322     0.000000     0.000000   \n",
       "25%         0.00000         4.791679    13.000000     0.000000   \n",
       "50%         0.00000         4.946059    18.000000     1.000000   \n",
       "75%         0.00000         5.092938    24.000000     2.000000   \n",
       "max        26.00000         5.681429   642.000000    17.000000   \n",
       "\n",
       "       avg_word_sentence          POS  POS/total_words  prompt_words  \\\n",
       "count        1332.000000  1332.000000      1332.000000   1332.000000   \n",
       "mean           23.884687   420.596542         0.989935    198.913664   \n",
       "std            11.160020   170.985111         0.007308     82.729266   \n",
       "min             1.084112    35.647059         0.924771     14.000000   \n",
       "25%            19.142857   305.406284         0.987758    144.000000   \n",
       "50%            22.030331   406.982869         0.991572    193.000000   \n",
       "75%            26.048234   520.739458         0.994425    246.000000   \n",
       "max           303.000000  1158.984563         1.000000    669.000000   \n",
       "\n",
       "       prompt_words/total_words  synonym_words  synonym_words/total_words  \\\n",
       "count               1332.000000     1332.00000                1332.000000   \n",
       "mean                   0.469164      110.16967                   0.263846   \n",
       "std                    0.052466       43.96192                   0.038870   \n",
       "min                    0.288889       11.00000                   0.027299   \n",
       "25%                    0.435709       81.00000                   0.238423   \n",
       "50%                    0.465852      107.50000                   0.262872   \n",
       "75%                    0.500000      134.00000                   0.288277   \n",
       "max                    0.961207      355.00000                   0.465517   \n",
       "\n",
       "         unstemmed      stemmed        score  \n",
       "count  1332.000000  1332.000000  1332.000000  \n",
       "mean    468.987988   455.507508     3.427177  \n",
       "std     159.447449   155.751220     0.774275  \n",
       "min      48.000000    50.000000     1.000000  \n",
       "25%     361.000000   350.750000     3.000000  \n",
       "50%     463.000000   448.000000     3.000000  \n",
       "75%     581.000000   561.250000     4.000000  \n",
       "max     750.000000   750.000000     6.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essay_features.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f38270",
   "metadata": {},
   "source": [
    "***\n",
    "## 2. Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318011ab",
   "metadata": {},
   "source": [
    "There are 2 type of machine learning approaches: \n",
    "- Supervised Machine learning \n",
    "- Unsupervised machine learning <br>\n",
    "\n",
    "<b> Supervised machine learning </b> uses labelled data and the algorithms learn to predict the output from the input data. <br>\n",
    "<b> Labelled data </b> is a group of samples that has labels which identify its properties or characteristics.\n",
    "Labels can come from observations or asking specialists about the data. When new unlabelled data is given to the model, the model is able to predict a likely label for the data.\n",
    "\n",
    "There are 2 type of problems which use supervised machine learning: Classification and Regression. <br>\n",
    "- Classification is used when output variable is categorical data (which in our case, score is a categorical-ordinal data)\n",
    "- Regression is used when output variable is a continuous value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d63359c",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b2fc80",
   "metadata": {},
   "source": [
    "<b>Feature selection</b> is used to reduce the number of input variables by choosing those that are relevant to the target variable (the input variables which have a correlation with the output variable), thus these input variables are useful for a model to predict the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a26cca",
   "metadata": {},
   "source": [
    "There are several type of feature selection methods for supervised machine learning such as <b>Filter and Wrapper</b>.<br>\n",
    "We will be using the filter selection method, which uses statistical measures to evaluate the relationship between each input variable and the target variable. <br>\n",
    "Since our data has numerical input and categorical output, the technique we will be using is <b>Kendall’s rank coefficient</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52bbb5b",
   "metadata": {},
   "source": [
    "We import the library <b>scipy.stats</b> to calculate Kendall’s tau<br>\n",
    "The <b>kendalltau function</b> returns a value of -1 to 1. A value approaching -1 indicates that there is negative correlation and a value approaching +1 indicates a positive correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41251421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04ffc534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KendalltauResult(correlation=0.020404014642527515, pvalue=0.3365365839873953)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.kendalltau(x=essay_features['essayid'],y=essay_features['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7185022d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KendalltauResult(correlation=0.5662913993383564, pvalue=1.12074440073025e-156)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.kendalltau(x=essay_features['chars'],y=essay_features['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83b9fe06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KendalltauResult(correlation=0.5455409604156707, pvalue=2.282631844331847e-145)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.kendalltau(x=essay_features['words'],y=essay_features['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f18a181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KendalltauResult(correlation=0.44986484334829663, pvalue=8.888516380233097e-97)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.kendalltau(x=essay_features['commas'],y=essay_features['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c47bcf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KendalltauResult(correlation=0.2799489001226123, pvalue=3.974133761586599e-37)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.kendalltau(x=essay_features['apostrophes'],y=essay_features['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b131b0ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KendalltauResult(correlation=0.19851964868501998, pvalue=2.21907699545917e-15)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.kendalltau(x=essay_features['punctuations'],y=essay_features['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b4013c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KendalltauResult(correlation=0.2455979911658759, pvalue=6.028923704303055e-31)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.kendalltau(x=essay_features['avg_word_length'],y=essay_features['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bec99759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KendalltauResult(correlation=0.49154562216707187, pvalue=8.30502458081386e-115)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.kendalltau(x=essay_features['sentences'],y=essay_features['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8e510e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KendalltauResult(correlation=0.2496394645307749, pvalue=1.1720789275881243e-25)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.kendalltau(x=essay_features['questions'],y=essay_features['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbb63c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KendalltauResult(correlation=-0.02071340101424389, pvalue=0.3293877474131538)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.kendalltau(x=essay_features['avg_word_sentence'],y=essay_features['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2a632cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KendalltauResult(correlation=0.5461931394880581, pvalue=6.338625439016613e-146)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.kendalltau(x=essay_features['POS'],y=essay_features['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cc35360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KendalltauResult(correlation=0.20790478624388226, pvalue=1.2204822056438548e-22)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.kendalltau(x=essay_features['POS/total_words'],y=essay_features['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4023bfd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KendalltauResult(correlation=0.541249699102086, pvalue=7.627491789118741e-143)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.kendalltau(x=essay_features['prompt_words'],y=essay_features['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a4ec254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KendalltauResult(correlation=0.04172987733066745, pvalue=0.04938961379675058)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.kendalltau(x=essay_features['prompt_words/total_words'],y=essay_features['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be7fd971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KendalltauResult(correlation=0.46910891005258765, pvalue=1.9308321900552002e-107)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.kendalltau(x=essay_features['synonym_words'],y=essay_features['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc244837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KendalltauResult(correlation=-0.23775364489280706, pvalue=4.214515958724073e-29)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.kendalltau(x=essay_features['synonym_words/total_words'],y=essay_features['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e2dbefc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KendalltauResult(correlation=0.5555578679361636, pvalue=4.107544572054594e-150)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.kendalltau(x=essay_features['unstemmed'],y=essay_features['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a7bdbd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KendalltauResult(correlation=0.5573415908524881, pvalue=3.0493449550250838e-151)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.kendalltau(x=essay_features['stemmed'],y=essay_features['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3896d2d",
   "metadata": {},
   "source": [
    "We will take features with tau value above 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6405c1b",
   "metadata": {},
   "source": [
    "### Separate the features and the label (Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9da02ac",
   "metadata": {},
   "source": [
    "Since we are taking the features with tau value above 0.4, the features chosen are as given below. <br>\n",
    "Features: 'chars', 'words', 'commas', 'sentences', 'POS', 'prompt_words', 'synonym_words', 'unstemmed', 'stemmed' <br>\n",
    "Label: ‘score’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bab9be45",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = essay_features.iloc[:,[1,2,3,7,10,12,16,17]].values\n",
    "y = essay_features.iloc[:, 18].values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3404064",
   "metadata": {},
   "source": [
    "### Split dataset into training and test data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac3a601",
   "metadata": {},
   "source": [
    "We import <b>train_test_split</b> function from the sklearn library, and we will be using this function to split the data into training and testing data set. <br>\n",
    "The training data set will be used to train our machine learning models and the testing data set will be given to the model to make predictions. <br>\n",
    "We will split the data into 80% training and 20% testing data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0220b50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and test data set\n",
    "# Take a portion of the data or take from a “random” sample of the data\n",
    "# 80% for the training data and 20% for the testing data\n",
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X, y, test_size = 0.2, random_state = 0 \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3955afaa",
   "metadata": {},
   "source": [
    "***\n",
    "## 3. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36653628",
   "metadata": {},
   "source": [
    "### Binary and multi-class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96579b7a",
   "metadata": {},
   "source": [
    "<b>Binary-class Classification</b> refers to tasks with only 2 two class labels. For example thief detection (thief or not). <br>\n",
    "<b>Multi-class Classification</b> refers to tasks with more than 2 class labels. For example classification of music genre. <br>\n",
    "https://machinelearningmastery.com/types-of-classification-in-machine-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3e9e04",
   "metadata": {},
   "source": [
    "### Data Scaling / Normalising"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b7bffe",
   "metadata": {},
   "source": [
    "<b>Purpose of normalising / scaling of data</b><br>\n",
    "The reason why we normalise and scale our data is that machine learning algorithm works faster and better when features (input variables) have similar scale and are close to being normally distributed. <br> \n",
    "- <b>Scaling</b> refers to changing the range of the data so that the variables/features fit within a specific range (for example 0-1) and the shape remain unchanged. As a result, we are able to compare different variables.<br>\n",
    "- <b>Normalisation</b> changes your data so that it is normally distributed. It is needed when we use machine learning which assumes data to be normally distributed. <br>\n",
    "\n",
    "https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02\n",
    "\n",
    "In our case, we will be using SVM which does not assume a normally dsitributed data. Hence we will be doing <b>Scaling</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9ba85f",
   "metadata": {},
   "source": [
    "<b>Ways to do scaling </b><br>\n",
    "There are different way to do scaling, namely MinMaxScaler, RobustScaler and StandardScaler.<br>\n",
    "- <b>StandardScaler</b> results in distribution with standard deviation = variance = 1, and mean of distribution = 0. - <br>\n",
    "- <b>MinMaxScaler</b> is used when there is no outliers to reduce and results in distribution with range 0 to 1. <br>\n",
    "- <b>RobustScaler</b> is used when there are outliers and there is a need to reduce the outliers' influence.<br>\n",
    "After considering the options, the StandardScaler will be used as outliers cannot be confirmed. <br>\n",
    "https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13f45dc",
   "metadata": {},
   "source": [
    "We import the library <b>sklearn.preprocessing</b> to use the function StandardScaler to scale our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02c38a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e357c89",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469b8879",
   "metadata": {},
   "source": [
    "<b>Support vector machine (SVM) </b><br>\n",
    "SVM is an algorithm used for supervised machine learning. <br>\n",
    "SVM can be used as a classifier, support vector classification (SVC), or used as a regressor, support vector regression (SVR). <br>\n",
    "\n",
    "<b>Support vector classification (SVC)</b> <br>\n",
    "In SVC, the algorithm produces a line in an N-dimensional space (N = number of features), called hyperplane, between the data to separate them into different classes. For example, if there is 2 features the hyperplane is a line, if there is 3 features the hyperplane is a plane, etc. <br>\n",
    "It uses support vectors (data points that are close to the hyperplane and have higher influence towards the orientation of the hyperplane) to build the SVM. <br>\n",
    "Hyperplane is useful because it allows us to separate data that are not linearly separable. (By creating a new dimension/plane, the data becomes linearly separable, allowing for the optimal line to be drawn) <br>\n",
    "\n",
    "<b>Support vector regression (SVR) and comparison to linear regression </b> <br>\n",
    "In SVR, although this step is similar to linear regression in terms of using a line to regress data, the method is fundamentally different. While line regression minimizes error between predicted and actual values using the line of best fit, the hyperplane is fitted between a threshold of values called epsilon-insensitive tube, where only the points within the tube are considered. Because the points within the tube have the least error rate, this gives us a better fitting model.\n",
    "\n",
    "https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea231de1",
   "metadata": {},
   "source": [
    "<b>Kernel</b><br>\n",
    "SVM kernel is a function which transforms the input data space (low number of features) into a higher dimensional space (high number of features). <br>\n",
    "It acts as a shortcut to avoid complex calculations and allows us to form hyperplanes without raising the complexity of our model. <br>\n",
    "\n",
    "By using kernels, we can solve non-linear problems using linear classifiers through the formation of hyperplanes. <br>\n",
    "- Kernel function is used when data points cannot be linearly seperated. The SVM kernel solves this problem by introducing additional features (by adding new feature z (z=x^2+y^2) on existing x and y input features). <br>\n",
    "\n",
    "The popular Kernel function includes: linear, polynomial, radial basis function and sigmoid.\n",
    "\n",
    "https://techvidvan.com/tutorials/svm-kernel-functions/#:~:text=A%20kernel%20is%20a%20function,number%20of%20dimensions%20using%20kernels. <br>\n",
    "https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53fbfb1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(decision_function_shape='ovo', kernel='linear')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(decision_function_shape='ovo',kernel='linear')\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775bed5d",
   "metadata": {},
   "source": [
    "Predict the score using our model for the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13071255",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f883fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 3, 3, 2, 3, 4, 4, 4, 2, 4, 4, 3, 4, 3, 3, 4, 3, 4, 4, 4, 2, 3,\n",
       "       3, 3, 3, 4, 4, 4, 3, 4, 4, 3, 3, 4, 4, 3, 4, 4, 3, 3, 4, 2, 3, 3,\n",
       "       3, 3, 3, 3, 2, 3, 4, 3, 4, 4, 4, 4, 4, 3, 3, 4, 3, 4, 3, 3, 4, 4,\n",
       "       4, 4, 4, 4, 3, 3, 4, 4, 4, 3, 3, 4, 4, 3, 4, 3, 4, 3, 3, 4, 3, 3,\n",
       "       4, 4, 3, 4, 4, 2, 4, 3, 3, 4, 3, 2, 4, 4, 3, 4, 4, 3, 4, 4, 3, 4,\n",
       "       2, 3, 3, 3, 3, 4, 3, 4, 3, 3, 3, 3, 4, 4, 3, 3, 3, 4, 4, 4, 4, 4,\n",
       "       4, 3, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 3, 3, 4, 4, 4, 3, 4, 4,\n",
       "       4, 3, 2, 3, 4, 3, 4, 4, 3, 3, 3, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4,\n",
       "       4, 4, 4, 2, 3, 4, 3, 3, 3, 4, 3, 3, 3, 3, 3, 4, 3, 3, 2, 3, 4, 4,\n",
       "       3, 4, 3, 4, 4, 3, 3, 4, 3, 3, 3, 3, 4, 4, 3, 3, 3, 3, 4, 4, 3, 3,\n",
       "       4, 4, 4, 4, 4, 3, 2, 4, 3, 3, 3, 3, 4, 3, 3, 4, 4, 4, 4, 4, 4, 4,\n",
       "       3, 3, 3, 4, 4, 3, 4, 4, 3, 4, 3, 4, 4, 4, 2, 2, 2, 3, 3, 3, 4, 3,\n",
       "       3, 3, 4])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6342f8b",
   "metadata": {},
   "source": [
    "The above shows the predicted score for the input essay from the testing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25403f26",
   "metadata": {},
   "source": [
    "### Confusion matrix and Accuracy Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2562cf",
   "metadata": {},
   "source": [
    "Using the <b>accuracy_score function</b> from sklearn library to generate accuracy for our SVC model. <br>\n",
    "The formula for the accuracy score is <b>Accuracy = (TP+TN)/(TP+TN+FP+FN)</b><br>\n",
    "TP = True Positive <br>\n",
    "TN = True Negative<br>\n",
    "FP = False Positive <br>\n",
    "FN = False Negative<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f28915cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.7265917602996255\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy = \" + str(accuracy_score(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ea6f06",
   "metadata": {},
   "source": [
    "The model has an Accuracy of approximately 0.73 (for the testing dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b27a5c",
   "metadata": {},
   "source": [
    "<b>Confusion matrix</b><br>\n",
    "A confusion matrix summarises the prediction results of the model. It outputs the number of correct predictions of each class and the number of incorrect predictions of each class in the form of a square matrix. The number of rows and columns in a confusion matrix depends on the number of classes. <br>\n",
    "For example, since our data has 6 classes, our confusion matrix is a 6x6 matrix. Each row of the matrix corresponds to an actual class, while each column of the matrix corresponds to a predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "960c5528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  2,  0,  0,  0,  0],\n",
       "       [ 0, 12,  7,  1,  0,  0],\n",
       "       [ 0,  1, 91, 23,  0,  0],\n",
       "       [ 0,  0, 28, 91,  0,  0],\n",
       "       [ 0,  0,  1,  9,  0,  0],\n",
       "       [ 0,  0,  0,  1,  0,  0]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix \n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c70a762",
   "metadata": {},
   "source": [
    "From the confusion matrix printed above we can gather information about number of True Positive, True Negative, False Positive and False negative for each class (6 classes, which is score of 1 to 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f29e432",
   "metadata": {},
   "source": [
    "### Quadratic Weighted Kappa (QWK)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f541f710",
   "metadata": {},
   "source": [
    "<b>Quadratic Weighted Kappa</b> is calculated using values between the actual/expected values and the predicted value. The metric varies from 0 to 1. A value nearer to 1 indicates that prediction is close to the actual value. \n",
    "\n",
    "It is useful because it takes into consideration how far away the prediction is from the actual. It does this by creating a weight matrix, where the prediction that is further away from the actual will receive a lower score as compared to prediction that is nearer to the actual values. <br>\n",
    "Whereas for accuracy, when the actual is 2, it considers the prediction of 1 and a prediction of 1000 as equal. It counts how many prediction is true/false without considering how far away are these predictions are from the actual values. <br>\n",
    "\n",
    "Since the Quadratic Kappa Metric is the same as cohen kappa metric in Sci-kit learn when weights are set to 'Quadratic', I will be using the cohen kappa metric code and try to aim for a score > 0.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "20477aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6565754799849417"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.metrics.cohen_kappa_score(y_test, y_pred, weights = \"quadratic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63cf1e0",
   "metadata": {},
   "source": [
    "Here we can see that the QWK score for our model is approximately 0.66"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c21f417",
   "metadata": {},
   "source": [
    "***\n",
    "## Kaggle prediction and submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaa23e5",
   "metadata": {},
   "source": [
    "We will read the input from FIT1043-Essay-Features-Submission.csv and predict the essay score using our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c2c0188",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the data from FIT1043-Essay-Features-Submission.csv file\n",
    "kaggle = pd.read_csv('FIT1043-Essay-Features-Submission.csv')\n",
    "#Extract the features we are using\n",
    "kaggle = kaggle.iloc[:,[1,2,3,7,10,12,16,17]].values\n",
    "\n",
    "#Scaling\n",
    "kaggle = sc.transform(kaggle)\n",
    "#Predict the score\n",
    "kaggle_test = clf.predict(kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "24324335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 3, 3, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 3, 4, 4, 4, 4, 3, 3, 3, 3,\n",
       "       4, 4, 4, 4, 4, 4, 4, 3, 2, 4, 3, 3, 4, 3, 4, 4, 3, 3, 3, 3, 3, 3,\n",
       "       2, 3, 3, 4, 4, 3, 3, 4, 4, 4, 3, 4, 3, 3, 4, 4, 2, 3, 3, 4, 3, 3,\n",
       "       4, 4, 3, 4, 4, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 4, 3, 4, 2, 4, 4,\n",
       "       2, 3, 3, 3, 3, 3, 4, 3, 4, 4, 3, 3, 3, 4, 2, 4, 3, 4, 3, 3, 4, 4,\n",
       "       4, 3, 4, 4, 4, 3, 3, 4, 2, 3, 4, 4, 4, 3, 3, 4, 3, 4, 3, 2, 4, 4,\n",
       "       2, 3, 4, 3, 3, 4, 2, 3, 4, 4, 4, 3, 3, 4, 4, 3, 4, 4, 4, 3, 4, 3,\n",
       "       4, 3, 3, 3, 3, 4, 4, 3, 2, 3, 4, 4, 3, 3, 2, 4, 3, 4, 4, 3, 4, 3,\n",
       "       4, 3, 3, 4, 4, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 4, 3, 4, 3, 4, 4, 3,\n",
       "       3])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e32cc80",
   "metadata": {},
   "source": [
    "The above shows the predicted essay score for the kaggle submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62d8b03",
   "metadata": {},
   "source": [
    "The predicted essay score will be added to the 30856183-Jing-Wei-Loh-1.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fa251ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['score'] = kaggle_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "30b85033",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('30856183-Jing-Wei-Loh-1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5f6a12",
   "metadata": {},
   "source": [
    "***\n",
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c2288b",
   "metadata": {},
   "source": [
    "The prediction model that we have created is fairly accurate, with accuracy of approximately 0.70 and QWK score of approximately 0.63"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
